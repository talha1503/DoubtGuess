{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Chatbot_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmAAgh9PMMyr6KgkLgSsS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talha1503/DoubtGuess/blob/master/Model/Pytorch_Chatbot_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCFY8Qs738ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RlFF8m44Ehf",
        "colab_type": "code",
        "outputId": "2459c76f-63ac-45cc-cdaa-557f2d595ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "class Embedding():\n",
        "  '''\n",
        "  This embedding block will act as a lookup table.\n",
        "  The input here will be a word and the output will be an embedding of the word.\n",
        "  FastText will be our first choice. If not found in any others, generate a random embedding\n",
        "  Input:\n",
        "    word from the vocabulary\n",
        "  Otuput:\n",
        "    embedding from the pre-trained vector spaces.\n",
        "  '''\n",
        "  def __init__(self,embedding_fast_text,embedding_glove,embedding_word2vec):\n",
        "    self.fast_text = embedding_fast_text\n",
        "    self.glove = embedding_glove\n",
        "    self.w2v = embedding_word2vec\n",
        "\n",
        "  def lookup(self,word,dimensions):\n",
        "    if self.fast_text.get(word):\n",
        "      return fast_text.get(word)\n",
        "    elif self.glove.get(word):\n",
        "      return glove.get(word)\n",
        "    elif self.w2v.get(word):\n",
        "      return w2v.get(word)\n",
        "    else:\n",
        "      return torch.rand(1,dimensions,device=device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c526c4d1455b>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    elif self.glove.get(word)\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL5VIHrp4VOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Positional_Embeddings(nn.Module):\n",
        "  '''\n",
        "  \n",
        "  We need to create a positional encoding for our model since our input will always be embeddings of the words and nothing\n",
        "  with respect to their positions. Hence we use Vasmari et al's positional encoding. \n",
        "  Input : \n",
        "    Matrix of Sequence_Length X Dimensions\n",
        "  Output:\n",
        "    Embedding with positional encoding of the word.\n",
        "  '''\n",
        "  def __init__(self,dimensions,sentence):\n",
        "    super().__init__()\n",
        "    self.dimensions = dimensions\n",
        "    self.sentence = sentence\n",
        "\n",
        "  def forward(self):\n",
        "    embeddings = Embedding()\n",
        "    self.embeddings = torch.zeros(len(self.sentence),self.dimensions,device = device)  #creating a matrix of sequence length X model dimensions.\n",
        "    for index in range(len(self.sentence)):\n",
        "      self.embeddings[index] = embeddings.lookup(self.sentence[index])\n",
        "    self.embeddings = self.embeddings * math.sqrt(self.dimensions)      #Embeedings are made larger so that no effect of positional encodings.\n",
        "    positional_encoded_embedding = torch.zeros(len(self.sentence).self.dimensions,device = device)\n",
        "    for index in range(len(self.sentence)):         #we create the positional embeddings over here\n",
        "      for embedding_index in range(0,self.dimensions,2):\n",
        "        positional_encoded_embedding[index,2*embedding_index] = math.sin(index/math.pow(10000,(2*embedding_index)/dimensions))    \n",
        "        positional_encoded_embedding[index,2*embedding_index+1] = math.cos(index/math.pow(10000,(2*embedding_index)+1/dimensions))    \n",
        "    positional_encoded_embedding = self.embedding +  positional_encoded_embedding  #we add ositional embeddings to the sentence matrix\n",
        "    return positional_encoded_embedding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7CCSUfVBFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''\n",
        "    This is the multi-head attention block which also consists of the masked multi head attention part of the decoder. \n",
        "    In this block we will recieve 3 inputs -> queries, keys and values.\n",
        "    Our flow will be like this:\n",
        "\n",
        "      q->linear->split->scale\n",
        "                            \\\n",
        "                            matmul --> softmax ----> matmul --> merge-->output_linear\n",
        "                           /                        /\n",
        "          k->linear->split                         /     \n",
        "                                                  /\n",
        "                                v->linear->split                       \n",
        "\n",
        "  '''\n",
        "  def __init__(self,heads,input_dims,q_dims,k_dims,v_dims,output_dims,mask = None):  #q_dims == k_dims since we need dot product of both of them together.\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.q_linear = nn.Linear(input_dims,q_dims,bias= False)\n",
        "    self.k_linear = nn.Linear(input_dims,k_dims,bias= False)\n",
        "    self.v_linear = nn.Linear(input_dims,v_dims,bias= False)\n",
        "    self.output_linear = nn.Linear(v_dims,output_dims,bias= False)\n",
        "    self.heads = heads\n",
        "    self.scaling_factor = (k_dims//self.heads)**-0.5\n",
        "    self.mask = mask\n",
        "\n",
        "  def split_heads(self,input_):\n",
        "    if len(input_.shape) == 3:       #Input shape is [batch_size,sequence_length,hidden_dimensions]  --> Output shape should be [batch_size,heads,seq_len,hidden_dim/heads]\n",
        "      shape = input_.shape\n",
        "      input_ = input_.view(shape[0],self.heads,shape[1],shape[2]//self.heads)\n",
        "      return input_\n",
        "    elif len(input_.shape) == 2:      #Input shape is [sequence_length,hidden_dimensions] --> output shape should be [heads,seq_len,hidden_dim/heads]\n",
        "      shape = input_.shape\n",
        "      input_ = input_.view(self.heads,shape[0],shape[1]//self.heads)\n",
        "      return input_\n",
        "    else:\n",
        "      print(\"Invalid Shapes\")\n",
        "\n",
        "  def merge_heads(self,input_):\n",
        "    if len(input_.shape) == 3:\n",
        "      shape = input_.shape\n",
        "      input_ = input_.view(shape[0],shape[2]*self.heads)\n",
        "    elif len(input_.shape) == 4:\n",
        "      shape = input_.shape\n",
        "      input_ = input_.view(shape[0],shape[2],shape[3]*self.heads)\n",
        "\n",
        "  def forward(self,queries,keys,values):\n",
        "    queries = self.q_linear(queries)\n",
        "    values = self.v_linear(values)\n",
        "    keys = self.k_linear(keys)\n",
        "\n",
        "    queries = self.split_heads(queries)      # in case of no batches shape is --> [heads , seq_len , dims // heads]\n",
        "    values = self.split_heads(values)        # in case of batches shape is -->    [batches , heads,seq_len,dims//heads]\n",
        "    keys = self.split_heads(keys)\n",
        "\n",
        "    queries = self.queries * self.scaling_factor\n",
        "\n",
        "    if len(queries.shape) == 3:  # no batches\n",
        "      scores = torch.matmul(queries,keys.permute(0,2,1))\n",
        "      '''\n",
        "      Mask the scores. \n",
        "      if self.mask:\n",
        "  \n",
        "      '''\n",
        "      probabilities = F.softmax(scores,dim = 0)\n",
        "    elif len(queries.shape) == 4: #in case of batches\n",
        "      scores = torch.matmul(queries,keys.permute(0,1,3,2))\n",
        "      '''\n",
        "      Mask the scores. \n",
        "      if self.mask:\n",
        "  \n",
        "      '''\n",
        "      probabilities = F.softmax(scores,dim = 1)\n",
        "\n",
        "\n",
        "\n",
        "    z = torch.matmul(probabilities , values)\n",
        "    z = merge_heads(z)\n",
        "\n",
        "    outputs = self.output_linear(z)\n",
        "    return  outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgrMKpk-VGBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  '''\n",
        "  This is feedforward network consisting of three layers(1 input ,1 hidden and 1 output)\n",
        "  Each layer is of the config --> Linear -> ReLU -> Dropout\n",
        "  '''\n",
        "  def __init__(self,hidden_size,input_size,output_size,dropout_val):\n",
        "    super(FeedForwardNetwork,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.first_linear = nn.Linear(self.input_size,self.hidden_size)\n",
        "    self.second_linear = nn.Linear(self.hidden_size,self.hidden_size)\n",
        "    self.third_linear = nn.Linear(self.hidden_size,self.output_size)\n",
        "    self.dropout = nn.Dropout(dropout_val)\n",
        "    self.relu = nn.ReLu()\n",
        "\n",
        "  def forward(self,inputs)\n",
        "    output1 = self.first_linear(inputs)\n",
        "    output1_relu = self.relu(output1)\n",
        "    output1_relu_dropout = self.dropout(output1_relu)\n",
        "    output2 = self.second_linear(output1_relu_dropout)\n",
        "    output2_relu = self.relu(output2)\n",
        "    output2_relu_dropout = self.dropout(output2_relu)\n",
        "    output3 = self.third_linear(output2_relu_dropout)\n",
        "    output3_relu = self.relu(output3)\n",
        "    output3_relu_dropout = self.dropout(output3_relu)\n",
        "    return ouptut3_relu_dropout\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZLDGpNVVGz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  '''\n",
        "    Reference -> https://github.com/pytorch/pytorch/issues/1959\n",
        "  '''\n",
        "  def __init__(self,features,eps=1e-12):\n",
        "    super(LayerNorm,self).__init__()\n",
        "    self.beta = nn.Parameter(torch.ones(features))\n",
        "    self.gamma = nn.Paramerter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self):\n",
        "    mean = input_.mean(-1,keepdims = True)\n",
        "    std = input_.std(-1,keepdims = True)\n",
        "    norm = self.gamma*(input_-mean)/(self.eps+self.std) + self.beta\n",
        "    return norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIehXhwwVNQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder_Layer(nn.Module):\n",
        "  def __init__(self,hidden_size,q_dims,k_dims,v_dims,heads,input_dims_attention,output_dims_attention,dropout_ffn,dropout_enc):\n",
        "    self.q_dims = q_dims\n",
        "    self.v_dims = v_dims\n",
        "    self.k_dims = k_dims\n",
        "    self.hidden_size = hidden_size\n",
        "    self.heads = heads\n",
        "    self.input_dims_attention = input_dims_attention\n",
        "    self.output_dims_attention = output_dims_attention\n",
        "    self.dropout_ffn = dropout_ffn\n",
        "    \n",
        "    self.mha = MultiHeadAttention(self.heads,self.input_dims_attention,self.q_dims,self.k_dims,self.v_dims,self.output_dims_attention) \n",
        "    self.ffn = FeedForwardNetwork(self.hidden_size,self.hidden_size,self.hidden_size,self.dropout_ffn)\n",
        "\n",
        "    self.layernorm_mha = LayerNorm(self.hidden_size)\n",
        "    self.layernorm_ffn = LayerNorm(self.hidden_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_enc)\n",
        "\n",
        "  def forward(self,input_):\n",
        "    layer_normed_input = self.layernorm_mha(input_)\n",
        "    mha_output = self.mha(layer_normed_input,layer_normed_input,layer_normed_input)\n",
        "    input_conc_mha = mha_output + input_\n",
        "    input_conc_mha_drop = self.dropout(input_conc_mha)\n",
        "    layer_normed_ffn = self.layernorm_mha(input_conc_mha_drop)\n",
        "    ffn_output = self.ffn(layer_normed_ffn)\n",
        "    ffn_output_concat = ffn_output+input_conc_mha_drop\n",
        "    ffn_output_concat_dropout = self.dropout(ffn_output_concat)\n",
        "    return ffn_output_concat_dropout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgQB0FilVykP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder_Layer(nn.Module):\n",
        "  def __init__(self):\n",
        "    \n",
        "\n",
        "  def forward(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hSTDivzV5Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTx7hLy9V_cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self):\n",
        "    pass "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}